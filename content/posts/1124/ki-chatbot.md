---
title: "Ich mach was mit KI ;-) - der Chatbot"
description: Kleines Hackingprojekt
date: 2024-11-27
tags: ["KI", "chatbot", "node"]
layout: layouts/post.njk
---

In letzter Zeit werde ich st√§ndig gefragt: "Ihr macht doch was mit KI, oder? K√∂nnt ihr nicht mal einen Chatbot bauen?" Die Idee klingt simpel<!-- endOfPreview -->, aber wie so oft steckt der Teufel im Detail. Ein Chatbot, der tats√§chlich n√ºtzliche Antworten liefert, erfordert weit mehr als nur eine schnelle Integration mit einer KI-API. Es braucht eine durchdachte Datenbasis, eine skalierbare Architektur und eine performante Anbindung.

In diesem Artikel nehme ich euch mit auf die Reise, wie ich einen modularen Chatbot entwickelt habe. Von der Datenaufbereitung, √ºber die Speicherung in einer Chroma-Datenbank, bis hin zur Implementierung einer Fastify-API mit GPT-4 zeigen ich die Herausforderungen und Learnings aus der Praxis. Das Ziel: Einen Bot, der nicht nur Fragen beantwortet, sondern dies auch schnell und pr√§zise tut.

---

## Das Ziel

Das Ziel war es, einen Chatbot zu entwickeln, der aus einer vorhandenen Wissensbasis ‚Äì in meinem Fall aus einer Vielzahl von Dokumenten ‚Äì Fragen beantwortet. Die Hauptbestandteile dieses Projekts:

1. **Datenparsing**: Daten aus JSON- oder Textquellen extrahieren und aufbereiten.
2. **Chroma-DB aufsetzen und bespielen**: Die aufbereiteten Daten als Vektoren in Chroma speichern.
3. **Fastify API mit GPT-4**: Eine REST-API, die GPT-4 anbindet, um semantisch passende Antworten zu generieren.

![Image](/img/1124/chatbot.png "chatbot architektur")

---

## Phase 1: Datenparsing

Der erste Schritt war das Extrahieren und Aufbereiten der Daten. Die Daten kamen aus JSON-Dateien und enthielten Texte sowie Metadaten. Der Fokus lag darauf, die Daten so vorzubereiten, dass sie sp√§ter in kleine, zusammenh√§ngende Bl√∂cke aufgeteilt werden k√∂nnen.

Hier ein Ausschnitt der Funktion, die Texte aufteilt und Metadaten speichert:

```typescript
export function splitText(
    text: string,
    maxLength: number,
    overlap: number,
): string[] {
    const chunks = [];
    for (let i = 0; i < text.length; i += maxLength - overlap) {
        chunks.push(text.slice(i, i + maxLength));
    }
    return chunks;
}

export async function processData(sourcePath: string, outputPath: string) {
    const files = fs.readdirSync(sourcePath);
    for (const file of files) {
        const content = fs.readFileSync(path.join(sourcePath, file), "utf-8");
        const chunks = splitText(content, 1000, 200);
        fs.writeFileSync(
            path.join(outputPath, `${file}.json`),
            JSON.stringify(chunks),
        );
    }
}
```

---

## Phase 2: Chroma-DB aufsetzen und bespielen

Mit den verarbeiteten Daten war der n√§chste Schritt das Bespielen einer Chroma-Datenbank. Dies wird als eigenst√§ndiger Job ausgef√ºhrt, der regelm√§√üig √ºber einen Cronjob l√§uft. So bleiben die Daten stets aktuell, ohne dass die API gest√∂rt wird.

Hier ein Ausschnitt der Funktion, die die Daten in Chroma speichert:

```typescript
export async function storeEmbeddings(dataDir: string, collectionName: string) {
    const client = new ChromaClient({ path: "http://localhost:8000" });
    const collection = await client.createCollection({ name: collectionName });
    const embeddingModel = new OpenAIEmbeddings({
        openAIApiKey: process.env.OPENAI_API_KEY,
    });

    const files = fs.readdirSync(dataDir);
    for (const file of files) {
        const chunks = JSON.parse(
            fs.readFileSync(path.join(dataDir, file), "utf-8"),
        );
        const embeddings = await embeddingModel.embedMany(chunks);
        await collection.add({
            ids: chunks.map((_, i) => `${file}_${i}`),
            embeddings,
            documents: chunks,
            metadatas: chunks.map((_, i) => ({ source: file })),
        });
    }
}
```

---

## Phase 3: Fastify-API mit GPT-4

Die Fastify-API verbindet Chroma mit GPT-4, um Antworten aus den gespeicherten Daten zu generieren. Ein wichtiges Learning war, dass nicht jedes GPT-4-Modell gleicherma√üen geeignet ist. Einige Modelle sind deutlich schneller darin, Daten entgegenzunehmen und auf deren Grundlage eine Antwort zu formulieren. Durch das Experimentieren mit verschiedenen Modellen konnte ich die Antwortzeit der API von 8 Sekunden auf 2-3 Sekunden reduzieren.

Hier ist der zentrale Teil der API:

```typescript
fastify.post("/query", async (request, reply) => {
    const { query } = request.body;
    const client = new ChromaClient({ path: "http://localhost:8000" });
    const collection = await client.getCollection({ name: "documents" });

    const embeddings = new OpenAIEmbeddings({
        openAIApiKey: process.env.OPENAI_API_KEY,
    });
    const queryEmbedding = await embeddings.embedQuery(query);
    const results = await collection.query({
        queryEmbeddings: [queryEmbedding],
        nResults: 3,
    });

    const context = results.documents.join("\n\n");
    const llm = new ChatOpenAI({
        openAIApiKey: process.env.OPENAI_API_KEY,
        modelName: "gpt-4-turbo",
    });
    const response = await llm.invoke([
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: `Answer based on this context: ${context}` },
    ]);

    reply.send({ answer: response.text });
});
```

### **Beispiel: API in Aktion**

Die Fastify-API erm√∂glicht es, Fragen an den Chatbot zu stellen. Der Bot durchsucht die hinterlegte Datenbasis nach relevanten Informationen und liefert eine Antwort zusammen mit Metadaten (wie Links zu den Quellen).

#### **Anfrage:**

```bash
POST /chatbot
Content-Type: application/json

{
  "phrase": "Wo finde ich Lorem Ipsum"
}
```

---

#### **Antwort:**

```json
{
    "answer": "Du findest das unter Lorem Ipsum.",
    "meta": [
        {
            "title": "Seite 1",
            "url": "https://example.com/lorem-ipsum"
        },
        {
            "title": "Dokumentation zu Ipsum",
            "url": "https://docs.example.com/ipsum"
        }
    ]
}
```

---

## Lessons Learned

-   **Experimentieren mit Modellen:** Unterschiedliche GPT-Modelle haben verschiedene St√§rken. Modelle wie `gpt-4-turbo` erwiesen sich als schneller und effizienter f√ºr diese Art von Anwendung, ohne nennenswerte Qualit√§tseinbu√üen.
-   **Separate Datenjobs:** Das Sammeln und Bespielen der Daten l√§uft in einem separaten Job, der w√∂chentlich √ºber Cron ausgef√ºhrt wird. Dadurch bleibt die API schlank und performant.
-   **Chroma-DB als zuverl√§ssige Basis:** Die Kombination aus Chroma und OpenAI liefert semantisch pr√§zise Antworten, wenn die Daten gut vorbereitet sind.

---

## Fazit

Am Ende entstand ein modularer Chatbot, der eine semantische Suche √ºber Dokumente bietet und GPT-4 f√ºr pr√§zise Antworten nutzt. Der iterative Ansatz und das Feintuning ‚Äì etwa beim Datenhandling oder den verwendeten Modellen ‚Äì waren entscheidend, um die Geschwindigkeit und Genauigkeit zu optimieren.

---

Falls du den Chatbot selbst ausprobieren m√∂chtest, kannst du dir das fertige Produkt bald unter dieser URL anschauen [Scopevisio Onlinehilfe](https://help.scopevisio.com/).
Code gibts diesmal leider nicht. Aber ich stehe f√ºr alle Fragen bereit.

Viel Spa√ü beim Basteln! üòä

F√ºr Feedback bin ich immer dankbar.
Gerne an jacob@derkuba.de

Viele Gr√º√üe

Euer Kuba

<br/>

**Ausnahmsweise heute mal etwas POST SCRIPTUM:**

### **PS: API Keys in `.env`**

Es ist entscheidend, API-Schl√ºssel niemals direkt im Code zu hinterlegen. Stattdessen sollten sie in einer `.env`-Datei gespeichert werden, die lokal geladen wird und nicht ins Versionskontrollsystem (wie GitHub) eingecheckt wird. Beispiel:

```
OPENAI_API_KEY=your-secret-api-key
CHROMA_DB_PATH=http://localhost:8000
```

Vergiss nicht, die `.env`-Datei in die `.gitignore` aufzunehmen, um sie vor versehentlichem Pushen zu sch√ºtzen. API-Schl√ºssel in √∂ffentlichen Repositories k√∂nnen teuer werden ‚Äì sei es durch Missbrauch oder unautorisierte Nutzung. Sollte sowas mal passieren, sofort den Key invalidieren und einen neuen Schl√ºssel generieren. Historie l√∂schen alleine reicht nicht.

---

### **PPS: Visual Studio Code REST Client**

Ein weiteres hilfreiches Tool f√ºr API-Arbeiten ist der [REST Client von Huachao Mao](https://marketplace.visualstudio.com/items?itemName=humao.rest-client). Dieser VS Code-Extension erm√∂glicht es, HTTP-Requests direkt im Editor zu speichern und auszuf√ºhren. Du kannst beispielsweise folgende `.http`-Datei anlegen:

```http
POST http://localhost:3000/chatbot
Content-Type: application/json

{
  "phrase": "Wo finde ich Lorem Ipsum"
}
```

Die Vorteile liegen auf der Hand:

-   Schneller Zugriff: Du kannst API-Aufrufe direkt neben deinem Code dokumentieren und ausf√ºhren.
-   Flexibilit√§t: Alle wichtigen Requests bleiben Teil des Projekts, ohne dass externe Tools erforderlich sind.
-   Sicherheit: Schl√ºssel und sensible Daten k√∂nnen in Variablen ausgelagert werden, sodass sie nicht im Code erscheinen.

**Durchsage ENDE**
